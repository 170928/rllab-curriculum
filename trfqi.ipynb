{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Region Fitted-Q Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Using CGT for CGT compatibility mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 770 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CGT_COMPAT_MODE\"] = \"cgt\"\n",
    "#os.environ[\"THEANO_FLAGS\"] = \"device=gpu\"\n",
    "from sampler import parallel_sampler\n",
    "parallel_sampler.reset()\n",
    "parallel_sampler.init_pool(1)\n",
    "from mdp import AtariMDP\n",
    "from policy import EpsilonGreedyPolicy\n",
    "from qfunc import AtariRAMQFunction\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorfuse as theano\n",
    "import tensorfuse.tensor as T\n",
    "import scipy\n",
    "from misc.tensor_utils import flatten_tensors\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_experiment():\n",
    "    mdp = AtariMDP(rom_path=\"vendor/atari_roms/pong.bin\", obs_type=\"ram\")\n",
    "    qfunc = AtariRAMQFunction(mdp, hidden_sizes=[512, 256, 128])\n",
    "    eps_policy = EpsilonGreedyPolicy(qfunc, epsilon=1)\n",
    "    parallel_sampler.populate_task(mdp, eps_policy)\n",
    "    return mdp, qfunc, eps_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_samples(eps_policy, epsilon, n_samples):\n",
    "    eps_policy.epsilon = epsilon\n",
    "    return parallel_sampler.request_samples(eps_policy.get_param_values(), n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trust region formulation:\n",
    "\n",
    "At each iteration, we solve the optimization problem\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\min &&\\frac{1}{N} \\sum_{n=1}^N \\left[Q_\\theta(s_n, a_n) - y_n \\right]^2 \\cr\n",
    "s.t. && \\frac{1}{N} \\sum_{n=1}^N \\frac{\\left[Q_\\theta(s_n, a_n) - Q_{\\theta_{old}}(s_n, a_n)\\right]^2}{2\\sigma^2}  < \\delta\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $y_n = r_n + \\gamma\\max_{a'} Q_{\\theta_{old}}(s'_{n}, a')$ and $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^N \\left[Q_{\\theta_{old}}(s_n, a_n) - y_n\\right]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling functions...\n",
      "epsilon:  1.0\n",
      "forming samples...\n",
      "optimizing...\n",
      "penalty: 0\n",
      "loss before: 0.215750336647 after: 0.0261024385691\n",
      "reg before: 0.0 after: 2.03084945679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.244 sec\n",
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpRewMean: -19.5714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.275 sec\n",
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:  0.955\n",
      "forming samples...\n",
      "optimizing...\n",
      "penalty: 0\n",
      "loss before: 0.0236016716808 after: 0.0216105543077\n",
      "reg before: 0.0 after: 2.0114300251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.271 sec\n",
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpRewMean: -19.6153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.279 sec\n",
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:  0.91\n",
      "forming samples...\n",
      "optimizing...\n",
      "penalty: 0\n",
      "loss before: 0.0221971087158 after: 0.0209395941347\n",
      "reg before: 0.0 after: 1.39678692818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.227 sec\n",
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpRewMean: -20.5833333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.229 sec\n",
      "0%                          100%\n",
      "[##############################] | ETA[sec]: 0.000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:  0.865\n",
      "forming samples...\n",
      "optimizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 10.282 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4c46eb561bdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_param_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluate_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vals\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         )\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 188\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    189\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    190\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;34m'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-4c46eb561bdb>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mqfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_param_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrain_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "def new_train_vars(qfunc):\n",
    "    obs = qfunc.input_var\n",
    "    actions = T.ivector(\"actions\")\n",
    "    rewards = T.vector(\"rewards\")\n",
    "    terminate = T.vector(\"terminate\")\n",
    "    penalty = T.scalar(\"penalty\")\n",
    "    prev_qval = T.matrix(\"prev_qval\")\n",
    "    return dict(\n",
    "        obs=obs,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        terminate=terminate,\n",
    "        prev_qval=prev_qval,\n",
    "        penalty=penalty,\n",
    "    )\n",
    "\n",
    "def to_train_var_list(obs, actions, rewards, terminate, prev_qval, penalty):\n",
    "    return [obs, actions, rewards, terminate, prev_qval, penalty]\n",
    "\n",
    "def new_loss(qfunc, discount, obs, actions, rewards, terminate, prev_qval, penalty):\n",
    "    qval = qfunc.qval_var\n",
    "    N = obs.shape[0]\n",
    "    qsa = qval[T.arange(N), actions]\n",
    "    y = rewards + (1 - terminate) * discount * T.concatenate([T.max(prev_qval[1:], axis=1), np.array([0.0]).astype(theano.config.floatX)])\n",
    "    prev_qsa = prev_qval[T.arange(N), actions]\n",
    "    \n",
    "    loss = T.mean(T.square(qsa - y))\n",
    "    sigmasq = T.mean(T.square(prev_qsa - y))\n",
    "    reg = T.mean(T.square(qsa - prev_qsa))\n",
    "    reg_normalized = reg / (2*T.square(sigmasq))\n",
    "    reg_loss = loss + penalty * reg\n",
    "    return dict(\n",
    "        loss=loss,\n",
    "        reg=reg_normalized,\n",
    "        reg_loss=reg_loss,\n",
    "        sigmasq=sigmasq\n",
    "    )\n",
    "    \n",
    "\n",
    "samples_per_itr = 10000\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay_range = 20\n",
    "discount = 0.99\n",
    "\n",
    "mdp, qfunc, eps_policy = setup_experiment()\n",
    "\n",
    "train_vars = new_train_vars(qfunc)\n",
    "result_vars = new_loss(qfunc, discount, **train_vars)\n",
    "reg_loss_var = result_vars[\"reg_loss\"]\n",
    "loss_var = result_vars[\"loss\"]\n",
    "reg_var = result_vars[\"reg\"]\n",
    "\n",
    "grads_var = T.grad(reg_loss_var, qfunc.params)\n",
    "\n",
    "train_var_list = to_train_var_list(**train_vars)\n",
    "print \"compiling functions...\"\n",
    "f_loss = theano.function(train_var_list, [loss_var, reg_var, reg_loss_var], allow_input_downcast=True, on_unused_input='ignore')\n",
    "f_grads = theano.function(train_var_list, grads_var, allow_input_downcast=True, on_unused_input='ignore')\n",
    "\n",
    "def evaluate_loss(train_vals):\n",
    "    def evaluate(params):\n",
    "        qfunc.set_param_values(params)\n",
    "        loss, reg, reg_loss = f_loss(*train_vals)\n",
    "        return reg_loss.astype(np.float64)\n",
    "    return evaluate\n",
    "\n",
    "def evaluate_grad(train_vals):\n",
    "    def evaluate(params):\n",
    "        qfunc.set_param_values(params)\n",
    "        grad = f_grads(*train_vals)\n",
    "        flattened_grad = flatten_tensors(map(np.asarray, grad))\n",
    "        return flattened_grad.astype(np.float64)\n",
    "    return evaluate\n",
    "\n",
    "for itr in range(100):\n",
    "    cur_epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * itr / epsilon_decay_range)\n",
    "    print \"epsilon: \", cur_epsilon\n",
    "    paths = request_samples(eps_policy, cur_epsilon, samples_per_itr)\n",
    "\n",
    "    print \"forming samples...\"\n",
    "    observations = np.vstack([path[\"observations\"][:-1] for path in paths])\n",
    "    actions = np.concatenate([path[\"actions\"].reshape(-1) for path in paths])\n",
    "    rewards = np.concatenate([path[\"rewards\"].reshape(-1) for path in paths])\n",
    "    terminate = np.concatenate([np.append(np.zeros(len(path[\"rewards\"]) - 1), 1) for path in paths]).astype(int)\n",
    "\n",
    "    prev_qval = qfunc.compute_qval(observations)\n",
    "    train_vals = [observations, actions, rewards, terminate, prev_qval]\n",
    "    \n",
    "    loss_before, reg_before, _ = f_loss(*(train_vals + [0]))\n",
    "\n",
    "    cur_params = qfunc.get_param_values()\n",
    "    \n",
    "    print \"optimizing...\"\n",
    "    for penalty in [0]:\n",
    "        result = scipy.optimize.fmin_l_bfgs_b(\n",
    "            func=evaluate_loss(train_vals + [penalty]),\n",
    "            x0=qfunc.get_param_values(),\n",
    "            fprime=evaluate_grad(train_vals + [penalty]),\n",
    "            maxiter=20\n",
    "        )\n",
    "\n",
    "        loss_after, reg_after, _ = f_loss(*(train_vals + [penalty]))\n",
    "        print \"penalty:\", penalty\n",
    "        print \"loss before:\", loss_before, \"after:\", loss_after\n",
    "        print \"reg before:\", reg_before, \"after:\", reg_after\n",
    "        \n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    opt_params = qfunc.get_param_values()\n",
    "    \n",
    "    # test performance\n",
    "    test_paths = request_samples(eps_policy, 0, samples_per_itr)\n",
    "    avg_reward = np.mean([sum(path[\"rewards\"]) for path in test_paths])\n",
    "    print \"EpRewMean:\", avg_reward\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discount = 0.99\n",
    "# obs_var = qfunc.input_var\n",
    "# N = obs_var.shape[0]\n",
    "# actions_var = T.ivector(\"actions\")\n",
    "# rewards_var = T.vector(\"rewards\")\n",
    "# terminate_var = T.ivector(\"terminate\")\n",
    "# qval_var = qfunc.qval_var\n",
    "                  \n",
    "# qa = qval_var[T.arange(N), actions_var]\n",
    "\n",
    "# ys = rewards_var + discount * T.concatenate([qa[1:], np.array([0])])\n",
    "# loss = T.sum(T.square((1 - terminate_var) * (qa - ys))) / T.sum(1 - terminate_var)\n",
    "\n",
    "\n",
    "# all_vars = [obs_var, actions_var, rewards_var, terminate_var]\n",
    "# all_vals = [batch_observations, batch_actions, batch_rewards, batch_terminate]\n",
    "\n",
    "# grads = T.grad(loss, qfunc.params)\n",
    "\n",
    "\n",
    "# f_loss = theano.function(all_vars, loss, allow_input_downcast=True, on_unused_input='ignore')\n",
    "# f_grads = theano.function(all_vars, grads, allow_input_downcast=True, on_unused_input='ignore')\n",
    "# print f_loss(*all_vals)\n",
    "\n",
    "\n",
    "# def evaluate_cost(params):\n",
    "#     qfunc.set_param_values(params)\n",
    "#     return f_loss(*all_vals).astype(np.float64)\n",
    "\n",
    "# def evaluate_grad(params):\n",
    "#     qfunc.set_param_values(params)\n",
    "#     grads = f_grads(*all_vals)\n",
    "#     flattened_grads = flatten_tensors(map(np.asarray, grads))\n",
    "#     return flattened_grads.astype(np.float64)\n",
    "\n",
    "\n",
    "# result = scipy.optimize.fmin_l_bfgs_b(func=evaluate_cost, x0=qfunc.get_param_values(), fprime=evaluate_grad, maxiter=20)\n",
    "# opt_params = qfunc.get_param_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration, we sample a bunch of trajectories, and fit the q function according to the sampled trajectories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
